# -*- coding: utf-8 -*-
"""Cluster Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1es-XrRse2Hw4vlJ2Qo5gcZBSzS3jfekB

# **Data Mining and Discovery Report**
Prototype based clustering vs K-means clustering.

**1.Importing the Libraries**
"""

pip install -U scikit-fuzzy

pip install PrettyTable

import pandas as pd
import numpy as np
import seaborn as sns
import skfuzzy as fuzz
from scipy import stats
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from prettytable import PrettyTable
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score

"""**2.Data Loading and preprocessing**"""

csv_file_path = "/content/Sales_Transactions_Dataset_Weekly.csv"

# Reading the CSV file from the local path
df = pd.read_csv(csv_file_path)

# Display the first few rows of the DataFrame
df.head()

df.describe()

df.info()

df.columns[:55]

df.shape

"""Verifying if normalised values provided are correct."""

df.iloc[0:2, 0:53].set_index('Product_Code')

df.iloc[0:2,np.r_[0,-52:-1:1]].set_index('Product_Code')

df.iloc[0:2,0:53].set_index('Product_Code').apply(lambda x : (x-x.min())/(x.max()-x.min()), axis = 1)

#Checking for missing values
df = df.replace('?',np.NaN)

print('Number of instances = %d' % (df.shape[0]))
print('Number of attributes = %d' % (df.shape[1]))

print('Number of missing values:')
for col in df.columns:
    print('\t%s: %d' % (col,df[col].isna().sum()))

# Extracting the sales qty values as the input data
# The product codes are unique and will be used as index
col_num = [0]
col_num.extend(list(np.arange(1,53)))
sales_data = df.iloc[:,col_num]  \
      .set_index(keys = "Product_Code")
sales_data.head()

# creating strandard scaled version of the input data
ssc = StandardScaler()
ssc_sales_data = ssc.fit_transform(sales_data)

# Creating a DataFrame with standard scaled values
ssc_sales_data_df = pd.DataFrame(ssc_sales_data)
ssc_sales_data_df.head()

# Displaying the boxplot for better visualization
plt.figure(figsize=(20, 15))
sns.boxplot(data=ssc_sales_data_df)
plt.title('Boxplot for Numerical Data')
plt.savefig('Boxplot.png')
plt.show()

# Calculate Z-scores for each data point in the sales_data
z_scores = (ssc_sales_data_df - ssc_sales_data_df.mean()) / ssc_sales_data_df.std()

# Set a threshold for Z-scores to identify outliers
z_threshold = 3  # Adjust this threshold based on your data and analysis

# Identify outliers using the Z-scores
outliers = np.abs(z_scores) > z_threshold

# Display the count of outliers for each column
print("Outliers:\n", outliers.sum())

# Handling outliers: Remove or transform
# Option 1: Remove outliers
sales_data_no_outliers = ssc_sales_data_df[~outliers.any(axis=1)]

# Option 2: Transform outliers (replace with column-wise median)
sales_data_transformed = ssc_sales_data_df.mask(outliers, ssc_sales_data_df.median(axis=0), axis=1)

# Display the first few rows of the new DataFrames (optional)
print("\nSales Data without Outliers:")
print(sales_data_no_outliers.head())

print("\nSales Data with Transformed Outliers:")
print(sales_data_transformed.head())

#Checking for duplicate values
dups = sales_data_transformed.duplicated()
print('Number of duplicate rows = %d' % (dups.sum()))
dups.loc[[11,28]]

"""**2.Prototype Based Clustering**"""

# Set the number of clusters
n_clusters = 2

# Extract the values from the DataFrame
data = sales_data_transformed.values.T

# Perform FCM clustering
cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
    data, n_clusters, 2, error=0.005, maxiter=1000, init=None
)

# Assign cluster labels to each data point
cluster_membership = np.argmax(u, axis=0)

# Add cluster labels to the DataFrame
sales_data_transformed["Cluster"] = cluster_membership

# Display the DataFrame with cluster labels
print(sales_data_transformed.head())

# Create a Seaborn scatter plot with labels and legend
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data[0], y=data[2], hue=cluster_membership, palette="rocket", label="Cluster", s=100)
plt.title("FCM Clustering Results")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.savefig('FCM_Clustering.png', dpi = 300)
plt.show()

"""**3.K-means Clustering**

Elbow Method
"""

# Extract the values from the DataFrame
data = sales_data_transformed.values

# Set the range of k values to test
k_values = range(1, 11)  # You can adjust this range based on your needs

# Initialize an empty list to store the sum of squared distances for each k
sse = []

# Fit KMeans for each k and calculate sum of squared distances
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)  # Explicitly set n_init
    kmeans.fit(data)
    sse.append(kmeans.inertia_)

# Plot the elbow curve
plt.plot(k_values, sse, marker='o')
plt.title('Elbow Plot for K-Means Clustering')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Sum of Squared Distances')
plt.savefig('Elbow_plot.png', dpi = 300)
plt.show()

# Extract the values from the DataFrame
data = sales_data_transformed.values

# Optimal number of clusters based on the elbow plot (you need to set this based on the plot)
optimal_k = 2

# Fit KMeans with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)  # You can adjust the value of n_init as needed
cluster_labels = kmeans.fit_predict(data)

# Add cluster labels to the DataFrame
sales_data_transformed['Cluster'] = cluster_labels

# Display the DataFrame with cluster labels
print(sales_data_transformed.head())

# Create a Seaborn scatter plot with labels and legend
plt.figure(figsize=(11, 6))
sns.scatterplot(x=data[:, 0], y=data[:, 2], hue=cluster_labels, palette='hls', label='Cluster', s=100)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=200, linewidths=3, color='red', label='Centroids')
plt.title('K-Means Clustering Results')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.savefig('K-means_clustering_Seaborn.png')
plt.show()

# Compare FCM and K-means results
print("Fuzzy Partition Coefficient (FCM):", fpc)
print("Inertia (K-means):", kmeans.inertia_)

# Results Analysis
if fpc > kmeans.inertia_:
    print("FCM provides higher Fuzzy Partition Coefficient (FPC) indicating better fuzzy clustering.")
else:
    print("K-means provides lower Inertia indicating better crisp clustering.")

# FCM clustering evaluation metrics
silhouette_fcm = silhouette_score(data, cluster_membership, metric='euclidean')
davies_bouldin_fcm = davies_bouldin_score(data, cluster_membership)

# K-means clustering evaluation metrics
silhouette_kmeans = silhouette_score(data, cluster_labels, metric='euclidean')
davies_bouldin_kmeans = davies_bouldin_score(data, cluster_labels)

# Create a PrettyTable
table = PrettyTable(["Metric", "FCM", "K-means"])
table.add_row(["Silhouette Score", silhouette_fcm, silhouette_kmeans])
table.add_row(["Davies-Bouldin Index", davies_bouldin_fcm, davies_bouldin_kmeans])
table.add_row(["Fuzzy Partition Coefficient (FPC)", fpc, "-"])

# Print the table
print(table)